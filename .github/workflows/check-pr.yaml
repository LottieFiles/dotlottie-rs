name: Check PR

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  check-pr:
    if: github.head_ref != 'release'
    runs-on: ubuntu-latest
    env:
      CC: clang
      CXX: clang++
      AR: llvm-ar
      RANLIB: llvm-ranlib
      CFLAGS: -O2
      CXXFLAGS: -O2
      BINDGEN_EXTRA_CLANG_ARGS: "-I/usr/include"
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: true
          fetch-depth: 0

      - name: Install build dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential clang llvm-dev libclang-dev

      - name: Setup Rust
        uses: ningenMe/setup-rustup@v1.1.0

      - name: Setup Conan
        uses: turtlebrowser/get-conan@main

      - name: Build
        run: |
          cargo build --manifest-path=./dotlottie-rs/Cargo.toml --features="dev"
          cargo build --manifest-path=./dotlottie-ffi/Cargo.toml

      - name: Test
        run: |
          cargo test --manifest-path=./dotlottie-rs/Cargo.toml --features="dev" -- --test-threads=1
          cargo test --manifest-path=./dotlottie-ffi/Cargo.toml -- --test-threads=1

      - name: Lint
        run: |
          cargo clippy --manifest-path=./dotlottie-rs/Cargo.toml  --features="dev" --all-targets -- -D clippy::print_stdout
          cargo clippy --manifest-path=./dotlottie-ffi/Cargo.toml --all-targets -- -D clippy::print_stdout
        env:
          RUSTFLAGS: "-Dwarnings"

  benchmark:
    if: github.event_name == 'pull_request' && github.head_ref != 'release'
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: read
    env:
      CC: clang
      CXX: clang++
      AR: llvm-ar
      RANLIB: llvm-ranlib
      CFLAGS: -O2
      CXXFLAGS: -O2
      BINDGEN_EXTRA_CLANG_ARGS: "-I/usr/include"
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.base_ref }}
          submodules: true
          fetch-depth: 0

      - name: Install build dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential clang llvm-dev libclang-dev

      - name: Setup Rust
        uses: ningenMe/setup-rustup@v1.1.0

      - name: Setup Conan
        uses: turtlebrowser/get-conan@main

      - name: Cache cargo (base)
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-bench-base-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-bench-base-

      - name: Build base benchmarks
        run: |
          cargo build --manifest-path=./dotlottie-rs/Cargo.toml --features="dev" --release

      - name: Run benchmarks on base branch
        run: |
          cargo bench --manifest-path=./dotlottie-rs/Cargo.toml --features="dev" -- --save-baseline base

      - name: Checkout PR branch
        uses: actions/checkout@v4
        with:
          clean: false
          submodules: true

      - name: Cache cargo (PR)
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
          key: ${{ runner.os }}-bench-pr-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-bench-pr-

      - name: Build PR benchmarks
        run: |
          cargo build --manifest-path=./dotlottie-rs/Cargo.toml --features="dev" --release

      - name: Run benchmarks on PR branch
        run: |
          cargo bench --manifest-path=./dotlottie-rs/Cargo.toml --features="dev" -- --baseline base

      - name: Generate comparison report
        id: report
        run: |
          python3 << 'EOF'
          import json
          import os
          from pathlib import Path

          def analyze_criterion_output(criterion_dir):
              results = []
              
              for estimates_file in criterion_dir.rglob("*/change/estimates.json"):
                  bench_name = estimates_file.parent.parent.name
                  
                  try:
                      with open(estimates_file) as f:
                          data = json.load(f)
                      
                      mean_change = data['mean']['point_estimate']
                      mean_lower = data['mean']['confidence_interval']['lower_bound']
                      mean_upper = data['mean']['confidence_interval']['upper_bound']
                      
                      results.append({
                          'name': bench_name,
                          'change': mean_change,
                          'lower': mean_lower,
                          'upper': mean_upper
                      })
                  except (FileNotFoundError, json.JSONDecodeError, KeyError):
                      continue
              
              return results

          # Analyze both packages
          rs_results = analyze_criterion_output(Path("dotlottie-rs/target/criterion"))

          all_results = []
          if rs_results:
              all_results.extend([{**r, 'package': 'dotlottie-rs'} for r in rs_results])

          # Generate markdown report
          comment = "## üìä Benchmark Results\n\n"

          if not all_results:
              comment += "No benchmark comparisons found. Benchmarks may not be set up yet.\n"
              has_regression = False
          else:
              # Group by package
              by_package = {}
              for r in all_results:
                  pkg = r['package']
                  if pkg not in by_package:
                      by_package[pkg] = []
                  by_package[pkg].append(r)
              
              has_regression = False
              
              for pkg, results in sorted(by_package.items()):
                  comment += f"### {pkg}\n\n"
                  comment += "| Benchmark | Change | Confidence Interval | Status |\n"
                  comment += "|-----------|--------|---------------------|--------|\n"
                  
                  for result in sorted(results, key=lambda x: x['change'], reverse=True):
                      change_pct = result['change'] * 100
                      lower_pct = result['lower'] * 100
                      upper_pct = result['upper'] * 100
                      
                      # Determine status
                      if result['lower'] > 0.05:  # Significantly slower
                          status = "‚ö†Ô∏è Regression"
                          has_regression = True
                      elif result['upper'] < -0.05:  # Significantly faster
                          status = "üöÄ Improvement"
                      elif abs(result['change']) < 0.02:  # Within 2%
                          status = "‚úÖ No change"
                      else:
                          status = "‚ö° Slight change"
                      
                      comment += f"| `{result['name']}` | **{change_pct:+.2f}%** | [{lower_pct:+.2f}%, {upper_pct:+.2f}%] | {status} |\n"
                  
                  comment += "\n"
              
              if has_regression:
                  comment += "‚ö†Ô∏è **Warning:** Performance regressions detected (>5% slower with 95% confidence)!\n\n"
              
              comment += "<details>\n<summary>How to interpret these results</summary>\n\n"
              comment += "- **Change**: Estimated performance difference (negative = faster, positive = slower)\n"
              comment += "- **Confidence Interval**: 95% confidence bounds for the change\n"
              comment += "- **Regression**: Lower bound > +5% (confidently slower)\n"
              comment += "- **Improvement**: Upper bound < -5% (confidently faster)\n\n"
              comment += "Criterion uses statistical analysis to account for noise and provide reliable comparisons.\n"
              comment += "</details>\n"

          # Write outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"comment<<EOF\n{comment}\nEOF\n")
              f.write(f"has_regression={'true' if has_regression else 'false'}\n")
          EOF

      - name: Comment PR with results
        uses: actions/github-script@v7
        with:
          script: |
            const comment = `${{ steps.report.outputs.comment }}`;

            // Find existing benchmark comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('üìä Benchmark Results')
            );

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      - name: Upload benchmark reports
        uses: actions/upload-artifact@v4
        with:
          name: criterion-reports
          path: |
            dotlottie-rs/target/criterion/
          retention-days: 30

      - name: Check for regressions
        if: steps.report.outputs.has_regression == 'true'
        run: |
          echo "::warning::Performance regressions detected! Review the benchmark report above."
          # Uncomment the next line to fail CI on regressions:
          exit 1
